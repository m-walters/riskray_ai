import math
import os
import pickle
from typing import TYPE_CHECKING

import boto3
import numpy as np
import pandas
import pandas as pd
from pydicom.filebase import DicomBytesIO
from pydicom.filereader import dcmread
from skimage.transform import downscale_local_mean, resize as sk_resize
from sklearn.model_selection import train_test_split
from termcolor import colored

from .. dicom import meta_conversions as meta
from .. settings import AWS_KEY_ID, AWS_SECRET_ACCESS_KEY

if TYPE_CHECKING:
    from .. dicom.dataframes import DicomFilePath


def get_s3_connection():
    """
    Get AWS keys from environment
    """

    client = boto3.client(
        's3',
        aws_access_key_id=AWS_KEY_ID,
        aws_secret_access_key=AWS_SECRET_ACCESS_KEY
    )  # low-level functional API
    resource = boto3.resource(
        's3',
        aws_access_key_id=AWS_KEY_ID,
        aws_secret_access_key=AWS_SECRET_ACCESS_KEY
    )  # low-level functional API
    bucket = resource.Bucket('riskraydata')

    return client, bucket


def generate_LR_file_items(dicom_enum: "DicomFilePath", target_lbl: int) -> list:
    info = dicom_enum.value
    items = []

    client, bucket = get_s3_connection()

    left_count, right_count = 0, 0
    for obj in bucket.objects.filter(Prefix=info['left_prefix']):
        if obj.key not in info['rejects']:
            items.extend(
                [
                    {
                        "target": target_lbl,
                        "side": 'L',
                        "path": obj.key
                    }
                ]
            )
            left_count += 1

    for obj in bucket.objects.filter(Prefix=info['right_prefix']):
        if obj.key not in info['rejects']:
            items.extend(
                [
                    {
                        "target": target_lbl,
                        "side": 'R',
                        "path": obj.key
                    }
                ]
            )
            right_count += 1

    print(f"Found {left_count} left DICOMs and {right_count} right DICOMs")

    return items


def get_dicom(s3_client, path):
    obj = s3_client.get_object(Bucket='riskraydata', Key=path)
    dicom_data = obj['Body'].read()
    dicom_bytes = DicomBytesIO(dicom_data)
    return dcmread(dicom_bytes)


def get_dicom_image(dicom_dataset, side=None):
    # dicom_dataset can be object returned from get_dicom function above
    if side == 'R':
        p_array = np.fliplr(dicom_dataset.pixel_array)
    else:
        p_array = dicom_dataset.pixel_array

    return p_array


def set_dicom_image(dicom_dataset, array: np.ndarray):
    dicom_dataset.PixelData = array.tobytes()
    dicom_dataset.Rows, dicom_dataset.Columns = array.shape
    return dicom_dataset

# Meta data keys of interest to work with. Each tuple describes:
# - The attribute on the dicom to get
# - Whether this will be included in our vector
# - The conversion function that handles the incoming value
# - How zeros should be treated. (Sometimes it seems 0 is put in place of null-ish values)
#
# Values are all converted to floats (even integers)
# Check out https://dicom.innolitics.com/ciods/mr-image/general-image for info on keys
META_DATA_KEYS = [
    ('PatientAge', True, meta.patient_age, None),
    ('PatientSize', False, meta.dicom_float, None),  # Length in meters. I think all NaNs
    ('PatientSex', False, meta.patient_sex, 0.),  # All our data has no gender looks like
    ('PatientWeight', False, meta.dicom_float, None),  # In kg. Also all NaNs I believe
    ('PregnancyStatus', False, meta.pregnancy, 0.),  # All our data has no pregnancy
    ('KVP', True, meta.dicom_float, 0.),
    ('DistanceSourceToDetector', True, meta.dicom_float, 0.),
    ('ExposureTime', True, meta.dicom_int, None),
    ('Exposure', True, meta.dicom_int, None),  # In mAs. Integer
    ('SpatialResolution', True, meta.dicom_float, 0.)
]

def extract_meta_data(dicom_dataset):
    values = []
    for key in META_DATA_KEYS:
        if not key[1]:
            # Indicated to skip
            continue
        x = dicom_dataset.get(key[0])  # Get initial value
        if not x:
            # Use null default
            # values.append(key[3])
            # TF does not like NaNs
            values.append(0.)
            continue
        x = float(key[2](x))  # Apply the function, assert float
        values.append(x)
    return values

def process_dicom_items(dicom_items, save=False, save_path=None, normalize_each=True):
    """
    From a list of dicom items generated by generate_LR_file_items process generate the numpy image arrays.
    If pad = True, add padding based on the largest file of the set
    You can opt to save the results via pickling. The results will also be returned either way
    save_path is the full path to the file
    normalize_each sets the pixel values of each image to span [0,1]
    """
    if save and (save_path is None or not os.path.exists(save_path.rpartition("/")[0])):
        raise Exception(f"Cannot save to save_path '{save_path}'")

    client, bucket = get_s3_connection()

    total_lbl_0, total_lbl_1 = 0, 0
    # Pandas dataframe
    df = pd.DataFrame(columns=[
        'Target', 'PixelArray', 'RawArray', 'Height', 'Width', 'KVP',
        'ExposureTime', 'DistanceSourceToDetector', 'Max', 'Min', 'MetaAttrs',
    ])

    for item in dicom_items:
        target = item['target']
        path = item['path']
        side = item['side']

        try:
            obj = client.get_object(Bucket='riskraydata', Key=path)
        except Exception:
            print(f"Could not find object at {path}")
            continue

        try:
            dicom_data = obj['Body'].read()
            dicom_bytes = DicomBytesIO(dicom_data)
            dicom_dataset = dcmread(dicom_bytes)
            # TODO -- WE MAY WANT TO JUST KEEP THESE AS INTEGERS
            if side == 'R':
                p_array = np.fliplr(dicom_dataset.pixel_array).astype('float32')
            else:
                p_array = dicom_dataset.pixel_array.astype('float32')

            meta_series = pd.Series(extract_meta_data(dicom_dataset))
            Kv = dicom_dataset.KVP
            ms = dicom_dataset.ExposureTime
            SID = dicom_dataset.DistanceSourceToDetector
            if Kv != 0 and ms != 0 and SID != 0:
                # print('KV' + str(Kv))
                # print('ms' + str(ms))
                # print('SID' + str(SID))
                raw = p_array.copy()
                p_array = p_array * Kv * Kv * ms / (SID * SID) / 4096
                min, max = np.amax(p_array), np.amin(p_array)
                p_array = (p_array - min) / (max - min)
                tempdf = pd.DataFrame(
                    [[
                        target,
                        p_array,
                        raw,
                        p_array.shape[0],
                        p_array.shape[1],
                        Kv,
                        ms,
                        SID,
                        max,
                        min,
                        meta_series
                    ]],
                    columns=[
                        'Target', 'PixelArray', 'RawArray', 'Height', 'Width', 'KVP',
                        'ExposureTime', 'DistanceSourceToDetector', 'Max', 'Min', 'MetaAttrs'
                    ]
                )
                del dicom_bytes
                del dicom_data
                del dicom_dataset
                del p_array
                df = df.append(tempdf, ignore_index=True)

            if target == 0:
                total_lbl_0 += 1
            else:
                total_lbl_1 += 1
            print(colored('\u2713', 'green') + f" Found and Processed {path}")
        except Exception as e:
            print(f"\nCould not process file {path} due to the following error:")
            print(f"\t{e}")

    print(f"\nProcessed {total_lbl_0} Target 0 DICOMS")
    print(f"\nProcessed {total_lbl_1} Target 1 DICOMS")

    # SAVE
    if save:
        # Pickle the dataframe
        pkl_split = save_path.rpartition(".pkl")
        pkl_path = pkl_split[0] + pkl_split[1] if pkl_split[0] else pkl_split[-1] + ".pkl"
        df.to_pickle(pkl_path)
        print(f"Saved dataframe pkl:\n\t{pkl_path}")

    return df


def center_crop_df(df: pd.DataFrame, col, height, width, inplace=False) -> pd.DataFrame:
    def _center_crop(arr, crop_height, crop_width):
        """
        Crop from the center out such that final image is crop_width x crop_height dimensions,
        or smaller if already smaller
        """
        y, x = arr.shape[0], arr.shape[1]
        x_mid, y_mid = x // 2, y // 2

        left_x = max(0, x_mid - crop_width // 2)
        right_x = min(x, left_x + crop_width)

        bottom_y = max(0, y_mid - crop_height // 2)
        top_y = min(y, bottom_y + crop_height)

        return arr[bottom_y:top_y, left_x:right_x]

    new_col = df[col].apply(lambda arr: _center_crop(arr, height, width))
    if inplace:
        df[col] = new_col
        return df
    else:
        copy = df.copy(deep=True)
        copy[col] = new_col
        return copy


def pad_df(df: pd.DataFrame, col, height, width, inplace=False) -> pd.DataFrame:
    def _pad(arr, pad_height, pad_width):
        top_pad = max(0, math.ceil((pad_height - arr.shape[0]) / 2))
        bot_pad = max(0, math.floor((pad_height - arr.shape[0]) / 2))
        left_pad = max(0, math.ceil((pad_width - arr.shape[1]) / 2))
        right_pad = max(0, math.floor((pad_width - arr.shape[1]) / 2))
        return np.pad(arr, ((top_pad, bot_pad), (left_pad, right_pad)), 'constant')

    new_col = df[col].apply(lambda arr: _pad(arr, height, width))
    if inplace:
        df[col] = new_col
        return df
    else:
        copy = df.copy(deep=True)
        copy[col] = new_col
        return copy


def downscale(df: pd.DataFrame, col, factor, inplace=False) -> pd.DataFrame:
    def _downscale(arr, factor):
        return downscale_local_mean(arr, (factor, factor))

    new_col = df[col].apply(lambda arr: _downscale(arr, factor))
    if inplace:
        df[col] = new_col
        return df
    else:
        copy = df.copy(deep=True)
        copy[col] = new_col
        return copy


def resize(df: pd.DataFrame, col, outshape, inplace=False) -> pd.DataFrame:
    def _resize(arr, outshape):
        return sk_resize(arr, outshape, anti_aliasing=True)

    new_col = df[col].apply(lambda arr: _resize(arr, outshape))
    if inplace:
        df[col] = new_col
        return df
    else:
        copy = df.copy(deep=True)
        copy[col] = new_col
        return copy

def load_pickle(pkl_path):
    with open(pkl_path, 'rb') as pfile:
        return pickle.load(pfile)
